{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy\n",
      "  Downloading numpy-2.1.1-cp311-cp311-win_amd64.whl.metadata (59 kB)\n",
      "Downloading numpy-2.1.1-cp311-cp311-win_amd64.whl (12.9 MB)\n",
      "   ---------------------------------------- 0.0/12.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/12.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/12.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/12.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/12.9 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/12.9 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/12.9 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/12.9 MB 799.2 kB/s eta 0:00:16\n",
      "   - -------------------------------------- 0.5/12.9 MB 799.2 kB/s eta 0:00:16\n",
      "   - -------------------------------------- 0.5/12.9 MB 799.2 kB/s eta 0:00:16\n",
      "   --- ------------------------------------ 1.0/12.9 MB 709.1 kB/s eta 0:00:17\n",
      "   --- ------------------------------------ 1.0/12.9 MB 709.1 kB/s eta 0:00:17\n",
      "   ---- ----------------------------------- 1.3/12.9 MB 745.8 kB/s eta 0:00:16\n",
      "   ---- ----------------------------------- 1.3/12.9 MB 745.8 kB/s eta 0:00:16\n",
      "   ------- -------------------------------- 2.4/12.9 MB 1.1 MB/s eta 0:00:10\n",
      "   ------- -------------------------------- 2.4/12.9 MB 1.1 MB/s eta 0:00:10\n",
      "   -------- ------------------------------- 2.6/12.9 MB 1.1 MB/s eta 0:00:10\n",
      "   ----------- ---------------------------- 3.7/12.9 MB 1.4 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 3.7/12.9 MB 1.4 MB/s eta 0:00:07\n",
      "   --------------- ------------------------ 5.0/12.9 MB 1.6 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 5.8/12.9 MB 1.8 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 6.6/12.9 MB 1.9 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 7.6/12.9 MB 2.0 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 7.6/12.9 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 9.7/12.9 MB 2.4 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 10.5/12.9 MB 2.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 11.5/12.9 MB 2.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 11.5/12.9 MB 2.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.9/12.9 MB 2.7 MB/s eta 0:00:00\n",
      "Installing collected packages: numpy\n",
      "Successfully installed numpy-2.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.2.2-cp311-cp311-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>=1.23.2 in d:\\anaconda\\envs\\ml\\lib\\site-packages (from pandas) (2.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\anaconda\\envs\\ml\\lib\\site-packages (from pandas) (2.9.0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in d:\\anaconda\\envs\\ml\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading pandas-2.2.2-cp311-cp311-win_amd64.whl (11.6 MB)\n",
      "   ---------------------------------------- 0.0/11.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/11.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/11.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/11.6 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/11.6 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/11.6 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/11.6 MB 837.5 kB/s eta 0:00:14\n",
      "   --- ------------------------------------ 1.0/11.6 MB 1.2 MB/s eta 0:00:09\n",
      "   ---- ----------------------------------- 1.3/11.6 MB 1.3 MB/s eta 0:00:09\n",
      "   ------- -------------------------------- 2.1/11.6 MB 1.7 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 3.4/11.6 MB 2.5 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 5.2/11.6 MB 3.4 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 6.0/11.6 MB 3.7 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 6.6/11.6 MB 3.4 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 9.4/11.6 MB 4.4 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 9.4/11.6 MB 4.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.5/11.6 MB 4.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.6/11.6 MB 4.5 MB/s eta 0:00:00\n",
      "Downloading pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "Downloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Installing collected packages: pytz, tzdata, pandas\n",
      "Successfully installed pandas-2.2.2 pytz-2024.2 tzdata-2024.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy\n",
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 列表索引时间： 0.05579638481140137\n",
      "Numpy 数组索引时间： 0.0956425666809082\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# 创建大列表和大数组\n",
    "large_list = [ list(range(1000)) for _ in range(1000) ]\n",
    "large_array = np.array(large_list)\n",
    "\n",
    "# 测试 Python 列表的索引速度\n",
    "start = time.time()\n",
    "for _ in range(1000000):\n",
    "    element = large_list[999][999]\n",
    "end = time.time()\n",
    "print(\"Python 列表索引时间：\", end - start)\n",
    "\n",
    "# 测试 numpy 数组的索引速度\n",
    "start = time.time()\n",
    "for _ in range(1000000):\n",
    "    element = large_array[500,500]\n",
    "end = time.time()\n",
    "print(\"Numpy 数组索引时间：\", end - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(-2/2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def calhls(a,b,c,d,e,f,g,h,i):\n",
    "    # 创建一个 3x3 的矩阵\n",
    "    matrix = np.array([[a, b, c],\n",
    "                    [d, e, f],\n",
    "                    [g, h, i]])\n",
    "\n",
    "    # 计算矩阵的行列式\n",
    "    determinant = np.linalg.det(matrix)\n",
    "\n",
    "    print(\"矩阵的行列式是:\", determinant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "3\n",
      "2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for i in reversed(range(1,5)):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      " 0  -2  -3  -4  -5  -4  -3  -2  -1  \n",
      "-1   0   0   0   0   0   0   0   0  \n",
      " 0  -6   0   0   0   0   0  -6   0  \n",
      "-7   0  -7   0  -7   0  -7   0  -7  \n",
      " 0   0   0   0   0   0   0   0   0  \n",
      " 0   0   0   0   0   0   0   0   0  \n",
      " 7   0   7   0   7   0   7   0   7  \n",
      " 0   6   0   0   0   0   0   6   0  \n",
      " 0   0   0   0   0   0   0   0   0  \n",
      " 1   2   3   4   5   4   3   2   1  \n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "from ChineseChess import Table\n",
    "table1 = Table.Table()\n",
    "table1.place((0,0,1),(0,1,2),(3,2,5),(5,5,\"rju\"))\n",
    "#table1.show()\n",
    "table1.clear()\n",
    "table1.initial(1)\n",
    "#table1.show()\n",
    "table1.go((0,0,-1),(1,0))\n",
    "table1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      " 0  -2   0   0   0  -4   0  -2  -1  \n",
      "-1   0   0  -5   0   0   0   0   0  \n",
      " 0   0   0  -4  -3   0   0   0   0  \n",
      " 0   0  -7   0  -7   0  -7  -6  -7  \n",
      " 0   0   0   0   0   0  -3   0   0  \n",
      " 0  -7   0   0   0   0   0   0   0  \n",
      " 7   0   7   0   7   0   7   0   7  \n",
      " 0   6   0   0   0   0   0   6   0  \n",
      " 0   0   0   0   0   0   0   0   0  \n",
      " 1  -6   3   4   5   4   3   2   1  \n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(6, 1), (5, 0), (5, 2)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#table1.whereCanGo((3,0,-7))\n",
    "table1.go((5,0,-7),(5,1))\n",
    "table1.show()\n",
    "table1.whereCanGo((5,1,-7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calcross(a,b,c,d,e,f):\n",
    "    # 定义两个三维向量\n",
    "    vector_a = np.array([a, b, c])\n",
    "    vector_b = np.array([d, e, f])\n",
    "\n",
    "    # 计算两个向量的叉乘\n",
    "    cross_product = np.cross(vector_a, vector_b)\n",
    "\n",
    "    print(\"向量 a 和 b 的叉乘结果是:\", cross_product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "向量 a 和 b 的叉乘结果是: [-3 -3  1]\n"
     ]
    }
   ],
   "source": [
    "calcross(1,0,3,0,1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.1270\n"
     ]
    }
   ],
   "source": [
    "class SimpleNerualNetwork(nn.Module):\n",
    "    def __init__(self, in_dim : int, hidden_dim : int, out_dim : int):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim,hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim,out_dim),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n",
    "    \n",
    "\n",
    "batch_size = 256\n",
    "in_dim = 4\n",
    "out_dim = 1\n",
    "hidden_dim = 64\n",
    "my_net = SimpleNerualNetwork(in_dim,hidden_dim,out_dim).to(\"cuda\")\n",
    "#out_put = my_net(torch.randn(size= (batch_size, 2)).cuda())\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(my_net.parameters(),lr = 0.01)\n",
    "\n",
    "input_data = torch.randn(size=(batch_size,in_dim)).cuda()\n",
    "labels = torch.randn(size=(batch_size, out_dim)).cuda()\n",
    "\n",
    "output = my_net(input_data)\n",
    "\n",
    "loss = criterion(output,labels)\n",
    "\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "print(f\"loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Module.parameters at 0x000002260AC75E00>\n"
     ]
    }
   ],
   "source": [
    "print(my_net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "print(out_put.__class__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "None\n",
      "tensor(437.7410)\n"
     ]
    }
   ],
   "source": [
    "P = torch.randn((1024, 1024))\n",
    "print(P.requires_grad) # -> False\n",
    "P = torch.randn((1024, 1024), requires_grad=True)\n",
    "print(P.requires_grad) # -> True\n",
    "b = torch.randn((1024,), requires_grad=True)\n",
    "print(b.grad) # -> None\n",
    "\n",
    "relu = torch.nn.ReLU()\n",
    "x = torch.randn((32,1024))\n",
    "y = relu(x @ P + b)\n",
    "target = 3\n",
    "loss = torch.mean((y - target) ** 2).detach()\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [1/10], Loss: 1.0047\n",
      "Epoch [1/10], Step [2/10], Loss: 1.1696\n",
      "Epoch [1/10], Step [3/10], Loss: 0.9351\n",
      "Epoch [1/10], Step [4/10], Loss: 0.9020\n",
      "Epoch [1/10], Step [5/10], Loss: 1.1174\n",
      "Epoch [1/10], Step [6/10], Loss: 0.9117\n",
      "Epoch [1/10], Step [7/10], Loss: 0.8864\n",
      "Epoch [1/10], Step [8/10], Loss: 1.1104\n",
      "Epoch [1/10], Step [9/10], Loss: 1.0531\n",
      "Epoch [1/10], Step [10/10], Loss: 0.9679\n",
      "Epoch [2/10], Step [1/10], Loss: 1.0235\n",
      "Epoch [2/10], Step [2/10], Loss: 0.9868\n",
      "Epoch [2/10], Step [3/10], Loss: 0.9733\n",
      "Epoch [2/10], Step [4/10], Loss: 1.0202\n",
      "Epoch [2/10], Step [5/10], Loss: 0.9865\n",
      "Epoch [2/10], Step [6/10], Loss: 0.9824\n",
      "Epoch [2/10], Step [7/10], Loss: 0.9371\n",
      "Epoch [2/10], Step [8/10], Loss: 0.8362\n",
      "Epoch [2/10], Step [9/10], Loss: 1.0359\n",
      "Epoch [2/10], Step [10/10], Loss: 0.9407\n",
      "Epoch [3/10], Step [1/10], Loss: 0.9249\n",
      "Epoch [3/10], Step [2/10], Loss: 1.0429\n",
      "Epoch [3/10], Step [3/10], Loss: 1.1413\n",
      "Epoch [3/10], Step [4/10], Loss: 1.0265\n",
      "Epoch [3/10], Step [5/10], Loss: 1.0411\n",
      "Epoch [3/10], Step [6/10], Loss: 0.8789\n",
      "Epoch [3/10], Step [7/10], Loss: 1.0356\n",
      "Epoch [3/10], Step [8/10], Loss: 1.0809\n",
      "Epoch [3/10], Step [9/10], Loss: 1.0651\n",
      "Epoch [3/10], Step [10/10], Loss: 1.1002\n",
      "Epoch [4/10], Step [1/10], Loss: 1.0639\n",
      "Epoch [4/10], Step [2/10], Loss: 1.0020\n",
      "Epoch [4/10], Step [3/10], Loss: 0.7836\n",
      "Epoch [4/10], Step [4/10], Loss: 0.9789\n",
      "Epoch [4/10], Step [5/10], Loss: 0.9508\n",
      "Epoch [4/10], Step [6/10], Loss: 1.0169\n",
      "Epoch [4/10], Step [7/10], Loss: 1.0405\n",
      "Epoch [4/10], Step [8/10], Loss: 1.1255\n",
      "Epoch [4/10], Step [9/10], Loss: 1.0497\n",
      "Epoch [4/10], Step [10/10], Loss: 0.9966\n",
      "Epoch [5/10], Step [1/10], Loss: 1.0681\n",
      "Epoch [5/10], Step [2/10], Loss: 1.0178\n",
      "Epoch [5/10], Step [3/10], Loss: 1.1089\n",
      "Epoch [5/10], Step [4/10], Loss: 1.3430\n",
      "Epoch [5/10], Step [5/10], Loss: 1.0107\n",
      "Epoch [5/10], Step [6/10], Loss: 0.9718\n",
      "Epoch [5/10], Step [7/10], Loss: 1.0159\n",
      "Epoch [5/10], Step [8/10], Loss: 1.0272\n",
      "Epoch [5/10], Step [9/10], Loss: 1.1163\n",
      "Epoch [5/10], Step [10/10], Loss: 1.0056\n",
      "Epoch [6/10], Step [1/10], Loss: 0.8898\n",
      "Epoch [6/10], Step [2/10], Loss: 1.0920\n",
      "Epoch [6/10], Step [3/10], Loss: 0.9186\n",
      "Epoch [6/10], Step [4/10], Loss: 0.9541\n",
      "Epoch [6/10], Step [5/10], Loss: 0.9682\n",
      "Epoch [6/10], Step [6/10], Loss: 1.0048\n",
      "Epoch [6/10], Step [7/10], Loss: 1.0155\n",
      "Epoch [6/10], Step [8/10], Loss: 0.9201\n",
      "Epoch [6/10], Step [9/10], Loss: 1.0211\n",
      "Epoch [6/10], Step [10/10], Loss: 1.0326\n",
      "Epoch [7/10], Step [1/10], Loss: 0.8472\n",
      "Epoch [7/10], Step [2/10], Loss: 1.0660\n",
      "Epoch [7/10], Step [3/10], Loss: 0.8756\n",
      "Epoch [7/10], Step [4/10], Loss: 1.0030\n",
      "Epoch [7/10], Step [5/10], Loss: 0.9749\n",
      "Epoch [7/10], Step [6/10], Loss: 1.0031\n",
      "Epoch [7/10], Step [7/10], Loss: 0.9215\n",
      "Epoch [7/10], Step [8/10], Loss: 0.9882\n",
      "Epoch [7/10], Step [9/10], Loss: 0.9813\n",
      "Epoch [7/10], Step [10/10], Loss: 1.0742\n",
      "Epoch [8/10], Step [1/10], Loss: 0.9754\n",
      "Epoch [8/10], Step [2/10], Loss: 0.8473\n",
      "Epoch [8/10], Step [3/10], Loss: 0.8863\n",
      "Epoch [8/10], Step [4/10], Loss: 0.9498\n",
      "Epoch [8/10], Step [5/10], Loss: 0.9559\n",
      "Epoch [8/10], Step [6/10], Loss: 0.9301\n",
      "Epoch [8/10], Step [7/10], Loss: 0.9672\n",
      "Epoch [8/10], Step [8/10], Loss: 0.9040\n",
      "Epoch [8/10], Step [9/10], Loss: 0.9791\n",
      "Epoch [8/10], Step [10/10], Loss: 1.0104\n",
      "Epoch [9/10], Step [1/10], Loss: 0.9986\n",
      "Epoch [9/10], Step [2/10], Loss: 1.0381\n",
      "Epoch [9/10], Step [3/10], Loss: 0.7825\n",
      "Epoch [9/10], Step [4/10], Loss: 0.9854\n",
      "Epoch [9/10], Step [5/10], Loss: 0.9606\n",
      "Epoch [9/10], Step [6/10], Loss: 1.0420\n",
      "Epoch [9/10], Step [7/10], Loss: 0.9424\n",
      "Epoch [9/10], Step [8/10], Loss: 0.8522\n",
      "Epoch [9/10], Step [9/10], Loss: 1.2212\n",
      "Epoch [9/10], Step [10/10], Loss: 0.9856\n",
      "Epoch [10/10], Step [1/10], Loss: 0.8313\n",
      "Epoch [10/10], Step [2/10], Loss: 0.9373\n",
      "Epoch [10/10], Step [3/10], Loss: 0.9674\n",
      "Epoch [10/10], Step [4/10], Loss: 0.8706\n",
      "Epoch [10/10], Step [5/10], Loss: 0.9408\n",
      "Epoch [10/10], Step [6/10], Loss: 1.0113\n",
      "Epoch [10/10], Step [7/10], Loss: 1.1093\n",
      "Epoch [10/10], Step [8/10], Loss: 1.1372\n",
      "Epoch [10/10], Step [9/10], Loss: 0.9580\n",
      "Epoch [10/10], Step [10/10], Loss: 0.9496\n",
      "训练完成！\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 定义神经网络模型\n",
    "class SimpleNeuralNetwork(nn.Module):\n",
    "    def __init__(self, in_dim: int, out_dim: int, hidden_dim: int):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, out_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n",
    "\n",
    "# 参数设置\n",
    "batch_size = 256\n",
    "in_dim = 2\n",
    "out_dim = 1  # 输出层大小\n",
    "hidden_dim = 64  # 隐藏层大小\n",
    "\n",
    "# 创建模型实例并移动到 GPU（如果可用）\n",
    "my_net = SimpleNeuralNetwork(in_dim, out_dim, hidden_dim).to(\"cuda\")\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.MSELoss()  # 均方误差损失函数，适用于回归问题\n",
    "optimizer = optim.SGD(my_net.parameters(), lr=0.01)  # 使用随机梯度下降优化器\n",
    "\n",
    "# 设置训练的总 epoch 数\n",
    "num_epochs = 10\n",
    "\n",
    "# 模拟数据加载器（实际应用中应使用 DataLoader）\n",
    "def data_loader(batch_size, num_batches):\n",
    "    for _ in range(num_batches):\n",
    "        yield (torch.randn(size=(batch_size, in_dim)).cuda(), \n",
    "               torch.randn(size=(batch_size, out_dim)).cuda())\n",
    "\n",
    "# 训练循环\n",
    "for epoch in range(num_epochs):\n",
    "    # 假设每个 epoch 有 10 个 batch\n",
    "    for i, (inputs, labels) in enumerate(data_loader(batch_size, 10)):\n",
    "        # 前向传播\n",
    "        output = my_net(inputs)\n",
    "        \n",
    "        # 计算损失\n",
    "        loss = criterion(output, labels)\n",
    "        \n",
    "        # 反向传播\n",
    "        optimizer.zero_grad()  # 清零所有参数的梯度缓存\n",
    "        loss.backward()  # 计算梯度\n",
    "        optimizer.step()  # 更新参数\n",
    "        \n",
    "        # 打印损失信息\n",
    "        if (i + 1) % 1 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/10], Loss: {loss.item():.4f}')\n",
    "\n",
    "print(\"训练完成！\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
