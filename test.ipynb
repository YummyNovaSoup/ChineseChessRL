{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy\n",
      "  Downloading numpy-2.1.1-cp311-cp311-win_amd64.whl.metadata (59 kB)\n",
      "Downloading numpy-2.1.1-cp311-cp311-win_amd64.whl (12.9 MB)\n",
      "   ---------------------------------------- 0.0/12.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/12.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/12.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/12.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/12.9 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/12.9 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/12.9 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/12.9 MB 799.2 kB/s eta 0:00:16\n",
      "   - -------------------------------------- 0.5/12.9 MB 799.2 kB/s eta 0:00:16\n",
      "   - -------------------------------------- 0.5/12.9 MB 799.2 kB/s eta 0:00:16\n",
      "   --- ------------------------------------ 1.0/12.9 MB 709.1 kB/s eta 0:00:17\n",
      "   --- ------------------------------------ 1.0/12.9 MB 709.1 kB/s eta 0:00:17\n",
      "   ---- ----------------------------------- 1.3/12.9 MB 745.8 kB/s eta 0:00:16\n",
      "   ---- ----------------------------------- 1.3/12.9 MB 745.8 kB/s eta 0:00:16\n",
      "   ------- -------------------------------- 2.4/12.9 MB 1.1 MB/s eta 0:00:10\n",
      "   ------- -------------------------------- 2.4/12.9 MB 1.1 MB/s eta 0:00:10\n",
      "   -------- ------------------------------- 2.6/12.9 MB 1.1 MB/s eta 0:00:10\n",
      "   ----------- ---------------------------- 3.7/12.9 MB 1.4 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 3.7/12.9 MB 1.4 MB/s eta 0:00:07\n",
      "   --------------- ------------------------ 5.0/12.9 MB 1.6 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 5.8/12.9 MB 1.8 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 6.6/12.9 MB 1.9 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 7.6/12.9 MB 2.0 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 7.6/12.9 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 9.7/12.9 MB 2.4 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 10.5/12.9 MB 2.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 11.5/12.9 MB 2.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 11.5/12.9 MB 2.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.9/12.9 MB 2.7 MB/s eta 0:00:00\n",
      "Installing collected packages: numpy\n",
      "Successfully installed numpy-2.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.2.2-cp311-cp311-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>=1.23.2 in d:\\anaconda\\envs\\ml\\lib\\site-packages (from pandas) (2.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\anaconda\\envs\\ml\\lib\\site-packages (from pandas) (2.9.0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in d:\\anaconda\\envs\\ml\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading pandas-2.2.2-cp311-cp311-win_amd64.whl (11.6 MB)\n",
      "   ---------------------------------------- 0.0/11.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/11.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/11.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/11.6 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/11.6 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/11.6 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/11.6 MB 837.5 kB/s eta 0:00:14\n",
      "   --- ------------------------------------ 1.0/11.6 MB 1.2 MB/s eta 0:00:09\n",
      "   ---- ----------------------------------- 1.3/11.6 MB 1.3 MB/s eta 0:00:09\n",
      "   ------- -------------------------------- 2.1/11.6 MB 1.7 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 3.4/11.6 MB 2.5 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 5.2/11.6 MB 3.4 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 6.0/11.6 MB 3.7 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 6.6/11.6 MB 3.4 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 9.4/11.6 MB 4.4 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 9.4/11.6 MB 4.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.5/11.6 MB 4.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.6/11.6 MB 4.5 MB/s eta 0:00:00\n",
      "Downloading pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "Downloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Installing collected packages: pytz, tzdata, pandas\n",
      "Successfully installed pandas-2.2.2 pytz-2024.2 tzdata-2024.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy\n",
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 列表索引时间： 0.05579638481140137\n",
      "Numpy 数组索引时间： 0.0956425666809082\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# 创建大列表和大数组\n",
    "large_list = [ list(range(1000)) for _ in range(1000) ]\n",
    "large_array = np.array(large_list)\n",
    "\n",
    "# 测试 Python 列表的索引速度\n",
    "start = time.time()\n",
    "for _ in range(1000000):\n",
    "    element = large_list[999][999]\n",
    "end = time.time()\n",
    "print(\"Python 列表索引时间：\", end - start)\n",
    "\n",
    "# 测试 numpy 数组的索引速度\n",
    "start = time.time()\n",
    "for _ in range(1000000):\n",
    "    element = large_array[500,500]\n",
    "end = time.time()\n",
    "print(\"Numpy 数组索引时间：\", end - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(-2/2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def calhls(a,b,c,d,e,f,g,h,i):\n",
    "    # 创建一个 3x3 的矩阵\n",
    "    matrix = np.array([[a, b, c],\n",
    "                    [d, e, f],\n",
    "                    [g, h, i]])\n",
    "\n",
    "    # 计算矩阵的行列式\n",
    "    determinant = np.linalg.det(matrix)\n",
    "\n",
    "    print(\"矩阵的行列式是:\", determinant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "3\n",
      "2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for i in reversed(range(1,5)):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      " 0  -2  -3  -4  -5  -4  -3  -2  -1  \n",
      "-1   0   0   0   0   0   0   0   0  \n",
      " 0  -6   0   0   0   0   0  -6   0  \n",
      "-7   0  -7   0  -7   0  -7   0  -7  \n",
      " 0   0   0   0   0   0   0   0   0  \n",
      " 0   0   0   0   0   0   0   0   0  \n",
      " 7   0   7   0   7   0   7   0   7  \n",
      " 0   6   0   0   0   0   0   6   0  \n",
      " 0   0   0   0   0   0   0   0   0  \n",
      " 1   2   3   4   5   4   3   2   1  \n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "from ChineseChess import Table\n",
    "table1 = Table.Table()\n",
    "table1.place((0,0,1),(0,1,2),(3,2,5),(5,5,\"rju\"))\n",
    "#table1.show()\n",
    "table1.clear()\n",
    "table1.initial(1)\n",
    "#table1.show()\n",
    "table1.go((0,0,-1),(1,0))\n",
    "table1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      " 0  -2   0   0   0  -4   0  -2  -1  \n",
      "-1   0   0  -5   0   0   0   0   0  \n",
      " 0   0   0  -4  -3   0   0   0   0  \n",
      " 0   0  -7   0  -7   0  -7  -6  -7  \n",
      " 0   0   0   0   0   0  -3   0   0  \n",
      " 0  -7   0   0   0   0   0   0   0  \n",
      " 7   0   7   0   7   0   7   0   7  \n",
      " 0   6   0   0   0   0   0   6   0  \n",
      " 0   0   0   0   0   0   0   0   0  \n",
      " 1  -6   3   4   5   4   3   2   1  \n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(6, 1), (5, 0), (5, 2)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#table1.whereCanGo((3,0,-7))\n",
    "table1.go((5,0,-7),(5,1))\n",
    "table1.show()\n",
    "table1.whereCanGo((5,1,-7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calcross(a,b,c,d,e,f):\n",
    "    # 定义两个三维向量\n",
    "    vector_a = np.array([a, b, c])\n",
    "    vector_b = np.array([d, e, f])\n",
    "\n",
    "    # 计算两个向量的叉乘\n",
    "    cross_product = np.cross(vector_a, vector_b)\n",
    "\n",
    "    print(\"向量 a 和 b 的叉乘结果是:\", cross_product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(201, 0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = [ [(0+i*67,0+j*67) for i in range(9)] for j in range(10)]\n",
    "l[0][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "False*1>0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.1270\n"
     ]
    }
   ],
   "source": [
    "class SimpleNerualNetwork(nn.Module):\n",
    "    def __init__(self, in_dim : int, hidden_dim : int, out_dim : int):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim,hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim,out_dim),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n",
    "    \n",
    "\n",
    "batch_size = 256\n",
    "in_dim = 4\n",
    "out_dim = 1\n",
    "hidden_dim = 64\n",
    "my_net = SimpleNerualNetwork(in_dim,hidden_dim,out_dim).to(\"cuda\")\n",
    "#out_put = my_net(torch.randn(size= (batch_size, 2)).cuda())\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(my_net.parameters(),lr = 0.01)\n",
    "\n",
    "input_data = torch.randn(size=(batch_size,in_dim)).cuda()\n",
    "labels = torch.randn(size=(batch_size, out_dim)).cuda()\n",
    "\n",
    "output = my_net(input_data)\n",
    "\n",
    "loss = criterion(output,labels)\n",
    "\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "print(f\"loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Module.parameters at 0x000002260AC75E00>\n"
     ]
    }
   ],
   "source": [
    "print(my_net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "print(out_put.__class__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "None\n",
      "tensor(437.7410)\n"
     ]
    }
   ],
   "source": [
    "P = torch.randn((1024, 1024))\n",
    "print(P.requires_grad) # -> False\n",
    "P = torch.randn((1024, 1024), requires_grad=True)\n",
    "print(P.requires_grad) # -> True\n",
    "b = torch.randn((1024,), requires_grad=True)\n",
    "print(b.grad) # -> None\n",
    "\n",
    "relu = torch.nn.ReLU()\n",
    "x = torch.randn((32,1024))\n",
    "y = relu(x @ P + b)\n",
    "target = 3\n",
    "loss = torch.mean((y - target) ** 2).detach()\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [1/10], Loss: 1.0047\n",
      "Epoch [1/10], Step [2/10], Loss: 1.1696\n",
      "Epoch [1/10], Step [3/10], Loss: 0.9351\n",
      "Epoch [1/10], Step [4/10], Loss: 0.9020\n",
      "Epoch [1/10], Step [5/10], Loss: 1.1174\n",
      "Epoch [1/10], Step [6/10], Loss: 0.9117\n",
      "Epoch [1/10], Step [7/10], Loss: 0.8864\n",
      "Epoch [1/10], Step [8/10], Loss: 1.1104\n",
      "Epoch [1/10], Step [9/10], Loss: 1.0531\n",
      "Epoch [1/10], Step [10/10], Loss: 0.9679\n",
      "Epoch [2/10], Step [1/10], Loss: 1.0235\n",
      "Epoch [2/10], Step [2/10], Loss: 0.9868\n",
      "Epoch [2/10], Step [3/10], Loss: 0.9733\n",
      "Epoch [2/10], Step [4/10], Loss: 1.0202\n",
      "Epoch [2/10], Step [5/10], Loss: 0.9865\n",
      "Epoch [2/10], Step [6/10], Loss: 0.9824\n",
      "Epoch [2/10], Step [7/10], Loss: 0.9371\n",
      "Epoch [2/10], Step [8/10], Loss: 0.8362\n",
      "Epoch [2/10], Step [9/10], Loss: 1.0359\n",
      "Epoch [2/10], Step [10/10], Loss: 0.9407\n",
      "Epoch [3/10], Step [1/10], Loss: 0.9249\n",
      "Epoch [3/10], Step [2/10], Loss: 1.0429\n",
      "Epoch [3/10], Step [3/10], Loss: 1.1413\n",
      "Epoch [3/10], Step [4/10], Loss: 1.0265\n",
      "Epoch [3/10], Step [5/10], Loss: 1.0411\n",
      "Epoch [3/10], Step [6/10], Loss: 0.8789\n",
      "Epoch [3/10], Step [7/10], Loss: 1.0356\n",
      "Epoch [3/10], Step [8/10], Loss: 1.0809\n",
      "Epoch [3/10], Step [9/10], Loss: 1.0651\n",
      "Epoch [3/10], Step [10/10], Loss: 1.1002\n",
      "Epoch [4/10], Step [1/10], Loss: 1.0639\n",
      "Epoch [4/10], Step [2/10], Loss: 1.0020\n",
      "Epoch [4/10], Step [3/10], Loss: 0.7836\n",
      "Epoch [4/10], Step [4/10], Loss: 0.9789\n",
      "Epoch [4/10], Step [5/10], Loss: 0.9508\n",
      "Epoch [4/10], Step [6/10], Loss: 1.0169\n",
      "Epoch [4/10], Step [7/10], Loss: 1.0405\n",
      "Epoch [4/10], Step [8/10], Loss: 1.1255\n",
      "Epoch [4/10], Step [9/10], Loss: 1.0497\n",
      "Epoch [4/10], Step [10/10], Loss: 0.9966\n",
      "Epoch [5/10], Step [1/10], Loss: 1.0681\n",
      "Epoch [5/10], Step [2/10], Loss: 1.0178\n",
      "Epoch [5/10], Step [3/10], Loss: 1.1089\n",
      "Epoch [5/10], Step [4/10], Loss: 1.3430\n",
      "Epoch [5/10], Step [5/10], Loss: 1.0107\n",
      "Epoch [5/10], Step [6/10], Loss: 0.9718\n",
      "Epoch [5/10], Step [7/10], Loss: 1.0159\n",
      "Epoch [5/10], Step [8/10], Loss: 1.0272\n",
      "Epoch [5/10], Step [9/10], Loss: 1.1163\n",
      "Epoch [5/10], Step [10/10], Loss: 1.0056\n",
      "Epoch [6/10], Step [1/10], Loss: 0.8898\n",
      "Epoch [6/10], Step [2/10], Loss: 1.0920\n",
      "Epoch [6/10], Step [3/10], Loss: 0.9186\n",
      "Epoch [6/10], Step [4/10], Loss: 0.9541\n",
      "Epoch [6/10], Step [5/10], Loss: 0.9682\n",
      "Epoch [6/10], Step [6/10], Loss: 1.0048\n",
      "Epoch [6/10], Step [7/10], Loss: 1.0155\n",
      "Epoch [6/10], Step [8/10], Loss: 0.9201\n",
      "Epoch [6/10], Step [9/10], Loss: 1.0211\n",
      "Epoch [6/10], Step [10/10], Loss: 1.0326\n",
      "Epoch [7/10], Step [1/10], Loss: 0.8472\n",
      "Epoch [7/10], Step [2/10], Loss: 1.0660\n",
      "Epoch [7/10], Step [3/10], Loss: 0.8756\n",
      "Epoch [7/10], Step [4/10], Loss: 1.0030\n",
      "Epoch [7/10], Step [5/10], Loss: 0.9749\n",
      "Epoch [7/10], Step [6/10], Loss: 1.0031\n",
      "Epoch [7/10], Step [7/10], Loss: 0.9215\n",
      "Epoch [7/10], Step [8/10], Loss: 0.9882\n",
      "Epoch [7/10], Step [9/10], Loss: 0.9813\n",
      "Epoch [7/10], Step [10/10], Loss: 1.0742\n",
      "Epoch [8/10], Step [1/10], Loss: 0.9754\n",
      "Epoch [8/10], Step [2/10], Loss: 0.8473\n",
      "Epoch [8/10], Step [3/10], Loss: 0.8863\n",
      "Epoch [8/10], Step [4/10], Loss: 0.9498\n",
      "Epoch [8/10], Step [5/10], Loss: 0.9559\n",
      "Epoch [8/10], Step [6/10], Loss: 0.9301\n",
      "Epoch [8/10], Step [7/10], Loss: 0.9672\n",
      "Epoch [8/10], Step [8/10], Loss: 0.9040\n",
      "Epoch [8/10], Step [9/10], Loss: 0.9791\n",
      "Epoch [8/10], Step [10/10], Loss: 1.0104\n",
      "Epoch [9/10], Step [1/10], Loss: 0.9986\n",
      "Epoch [9/10], Step [2/10], Loss: 1.0381\n",
      "Epoch [9/10], Step [3/10], Loss: 0.7825\n",
      "Epoch [9/10], Step [4/10], Loss: 0.9854\n",
      "Epoch [9/10], Step [5/10], Loss: 0.9606\n",
      "Epoch [9/10], Step [6/10], Loss: 1.0420\n",
      "Epoch [9/10], Step [7/10], Loss: 0.9424\n",
      "Epoch [9/10], Step [8/10], Loss: 0.8522\n",
      "Epoch [9/10], Step [9/10], Loss: 1.2212\n",
      "Epoch [9/10], Step [10/10], Loss: 0.9856\n",
      "Epoch [10/10], Step [1/10], Loss: 0.8313\n",
      "Epoch [10/10], Step [2/10], Loss: 0.9373\n",
      "Epoch [10/10], Step [3/10], Loss: 0.9674\n",
      "Epoch [10/10], Step [4/10], Loss: 0.8706\n",
      "Epoch [10/10], Step [5/10], Loss: 0.9408\n",
      "Epoch [10/10], Step [6/10], Loss: 1.0113\n",
      "Epoch [10/10], Step [7/10], Loss: 1.1093\n",
      "Epoch [10/10], Step [8/10], Loss: 1.1372\n",
      "Epoch [10/10], Step [9/10], Loss: 0.9580\n",
      "Epoch [10/10], Step [10/10], Loss: 0.9496\n",
      "训练完成！\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 定义神经网络模型\n",
    "class SimpleNeuralNetwork(nn.Module):\n",
    "    def __init__(self, in_dim: int, out_dim: int, hidden_dim: int):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, out_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n",
    "\n",
    "# 参数设置\n",
    "batch_size = 256\n",
    "in_dim = 2\n",
    "out_dim = 1  # 输出层大小\n",
    "hidden_dim = 64  # 隐藏层大小\n",
    "\n",
    "# 创建模型实例并移动到 GPU（如果可用）\n",
    "my_net = SimpleNeuralNetwork(in_dim, out_dim, hidden_dim).to(\"cuda\")\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.MSELoss()  # 均方误差损失函数，适用于回归问题\n",
    "optimizer = optim.SGD(my_net.parameters(), lr=0.01)  # 使用随机梯度下降优化器\n",
    "\n",
    "# 设置训练的总 epoch 数\n",
    "num_epochs = 10\n",
    "\n",
    "# 模拟数据加载器（实际应用中应使用 DataLoader）\n",
    "def data_loader(batch_size, num_batches):\n",
    "    for _ in range(num_batches):\n",
    "        yield (torch.randn(size=(batch_size, in_dim)).cuda(), \n",
    "               torch.randn(size=(batch_size, out_dim)).cuda())\n",
    "\n",
    "# 训练循环\n",
    "for epoch in range(num_epochs):\n",
    "    # 假设每个 epoch 有 10 个 batch\n",
    "    for i, (inputs, labels) in enumerate(data_loader(batch_size, 10)):\n",
    "        # 前向传播\n",
    "        output = my_net(inputs)\n",
    "        \n",
    "        # 计算损失\n",
    "        loss = criterion(output, labels)\n",
    "        \n",
    "        # 反向传播\n",
    "        optimizer.zero_grad()  # 清零所有参数的梯度缓存\n",
    "        loss.backward()  # 计算梯度\n",
    "        optimizer.step()  # 更新参数\n",
    "        \n",
    "        # 打印损失信息\n",
    "        if (i + 1) % 1 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/10], Loss: {loss.item():.4f}')\n",
    "\n",
    "print(\"训练完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: fc.weight, Shape: tensor([[ 0.1268,  0.0845,  0.0905, -0.1804,  0.2969, -0.2591,  0.2511,  0.0405,\n",
      "         -0.0306,  0.2479]])\n",
      "Name: fc.bias, Shape: tensor([0.3086])\n",
      "Name: fc.weight, Shape: tensor([[-0.2041,  0.3080,  0.1699,  0.0412, -0.0989,  0.2511,  0.2760,  0.0185,\n",
      "         -0.0323, -0.0299]])\n",
      "Name: fc.bias, Shape: tensor([-0.0431])\n",
      "Name: fc.weight, Shape: tensor([[ 0.1268,  0.0845,  0.0905, -0.1804,  0.2969, -0.2591,  0.2511,  0.0405,\n",
      "         -0.0306,  0.2479]])\n",
      "Name: fc.bias, Shape: tensor([0.3086])\n",
      "Name: fc.weight, Shape: tensor([[-0.2067,  0.3058,  0.1650,  0.0476, -0.1101,  0.2577,  0.2706,  0.0212,\n",
      "         -0.0323, -0.0395]])\n",
      "Name: fc.bias, Shape: tensor([-0.0532])\n",
      "Reference Loss: 1.9973565340042114\n",
      "Name: fc.weight, Shape: tensor([[ 0.1268,  0.0845,  0.0905, -0.1804,  0.2969, -0.2591,  0.2511,  0.0405,\n",
      "         -0.0306,  0.2479]])\n",
      "Name: fc.bias, Shape: tensor([0.3086])\n",
      "Name: fc.weight, Shape: tensor([[-0.2087,  0.3039,  0.1601,  0.0524, -0.1169,  0.2622,  0.2639,  0.0214,\n",
      "         -0.0304, -0.0486]])\n",
      "Name: fc.bias, Shape: tensor([-0.0609])\n",
      "Reference Loss: 1.7147681713104248\n",
      "Name: fc.weight, Shape: tensor([[ 0.1268,  0.0845,  0.0905, -0.1804,  0.2969, -0.2591,  0.2511,  0.0405,\n",
      "         -0.0306,  0.2479]])\n",
      "Name: fc.bias, Shape: tensor([0.3086])\n",
      "Name: fc.weight, Shape: tensor([[-0.2157,  0.3026,  0.1553,  0.0611, -0.1255,  0.2672,  0.2578,  0.0223,\n",
      "         -0.0281, -0.0563]])\n",
      "Name: fc.bias, Shape: tensor([-0.0679])\n",
      "Reference Loss: 1.6172573566436768\n",
      "Name: fc.weight, Shape: tensor([[ 0.1268,  0.0845,  0.0905, -0.1804,  0.2969, -0.2591,  0.2511,  0.0405,\n",
      "         -0.0306,  0.2479]])\n",
      "Name: fc.bias, Shape: tensor([0.3086])\n",
      "Name: fc.weight, Shape: tensor([[-0.2149,  0.3042,  0.1499,  0.0675, -0.1317,  0.2746,  0.2529,  0.0204,\n",
      "         -0.0252, -0.0631]])\n",
      "Name: fc.bias, Shape: tensor([-0.0775])\n",
      "Reference Loss: 1.6018776893615723\n",
      "Name: fc.weight, Shape: tensor([[ 0.1268,  0.0845,  0.0905, -0.1804,  0.2969, -0.2591,  0.2511,  0.0405,\n",
      "         -0.0306,  0.2479]])\n",
      "Name: fc.bias, Shape: tensor([0.3086])\n",
      "Name: fc.weight, Shape: tensor([[-0.2161,  0.3008,  0.1482,  0.0723, -0.1385,  0.2804,  0.2431,  0.0201,\n",
      "         -0.0259, -0.0716]])\n",
      "Name: fc.bias, Shape: tensor([-0.0840])\n",
      "Reference Loss: 1.7887749671936035\n",
      "Name: fc.weight, Shape: tensor([[-0.2161,  0.3008,  0.1482,  0.0723, -0.1385,  0.2804,  0.2431,  0.0201,\n",
      "         -0.0259, -0.0716]])\n",
      "Name: fc.bias, Shape: tensor([-0.0840])\n",
      "Name: fc.weight, Shape: tensor([[-0.2161,  0.3008,  0.1482,  0.0723, -0.1385,  0.2804,  0.2431,  0.0201,\n",
      "         -0.0259, -0.0716]])\n",
      "Name: fc.bias, Shape: tensor([-0.0840])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 假设我们有两个相同的模型架构\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, input_size=91, hidden_size=10000, output_size=90+90):\n",
    "        super(MyModel, self).__init__()\n",
    "        # input_size = 91\n",
    "        # hidden_size = 10000\n",
    "        # output_size = 90*90\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)  # 输入层到隐藏层\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size)  # 批量归一化\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)  # 隐藏层到隐藏层\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_size)  # 批量归一化\n",
    "        self.fc3_1 = nn.Linear(hidden_size, output_size//2)  # 隐藏层到输出层,分类一\n",
    "        self.fc3_2 = nn.Linear(hidden_size, output_size//2)  # 隐藏层到输出层,分类二\n",
    "        self.dropout = nn.Dropout(p=0.5)  # 丢弃层\n",
    "\n",
    "    def forward(self, x):\n",
    "        #作用于(turn,state)\n",
    "        #x = torch.cat(torch.tensor(x[1]))\n",
    "        x = torch.relu(self.bn1(self.fc1(x)))  # 输入层\n",
    "        x = self.dropout(x)  # 丢弃层\n",
    "        x = torch.relu(self.bn2(self.fc2(x)))  # 隐藏层\n",
    "        y1 = F.softmax(self.fc3_1(x),dim=1)  # 输出层\n",
    "        y2 = F.softmax(self.fc3_2(x),dim=1)\n",
    "        return y1,y2\n",
    "\n",
    "# 实例化两个模型\n",
    "reference_model = MyModel()\n",
    "target_model = MyModel()\n",
    "\n",
    "# 创建优化器\n",
    "optimizer_reference = optim.SGD(reference_model.parameters(), lr=0.01)\n",
    "optimizer_target = optim.SGD(target_model.parameters(), lr=0.01)\n",
    "\n",
    "# 假设我们有一些输入数据\n",
    "inputs = torch.randn(100, 91)\n",
    "labels1 = torch.randint(90, (100,))\n",
    "# 将模型设置为训练模式\n",
    "reference_model.train()  # 参考模型需要梯度计算\n",
    "target_model.train()     # 目标模型也需要梯度计算\n",
    "\n",
    "for name, param in reference_model.named_parameters():\n",
    "    print(f\"Name: {name}, Shape: {param.data}\")\n",
    "for name, param in target_model.named_parameters():\n",
    "    print(f\"Name: {name}, Shape: {param.data}\")\n",
    "for _ in range(5):\n",
    "    # 使用参考模型计算输出\n",
    "    reference_outputs = reference_model(inputs)\n",
    "\n",
    "    # 计算目标模型的输出\n",
    "    target_outputs = target_model(inputs)\n",
    "\n",
    "    # 假设我们有一个标准输出（例如来自标签）\n",
    "    standard_outputs = torch.randn(100, 1)\n",
    "\n",
    "    # 定义损失函数\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # 计算参考模型的损失\n",
    "    reference_loss = criterion(reference_outputs, standard_outputs)\n",
    "\n",
    "    # 反向传播计算参考模型的梯度\n",
    "    reference_loss.backward()\n",
    "\n",
    "    # 保存参考模型的梯度\n",
    "    gradients = []\n",
    "    for param in reference_model.parameters():\n",
    "        gradients.append(param.grad.clone())\n",
    "\n",
    "    # 清除参考模型的梯度\n",
    "    optimizer_reference.zero_grad()\n",
    "\n",
    "    # 清除目标模型的梯度\n",
    "    optimizer_target.zero_grad()\n",
    "\n",
    "    # 将参考模型的梯度应用到目标模型的参数上\n",
    "    for param, grad in zip(target_model.parameters(), gradients):\n",
    "        param.grad = grad\n",
    "\n",
    "    # 更新目标模型的参数\n",
    "    optimizer_target.step()\n",
    "\n",
    "    for name, param in reference_model.named_parameters():\n",
    "        print(f\"Name: {name}, Shape: {param.data}\")\n",
    "    for name, param in target_model.named_parameters():\n",
    "        print(f\"Name: {name}, Shape: {param.data}\")\n",
    "\n",
    "    # 打印损失以便监控训练过程\n",
    "    print(\"Reference Loss:\", reference_loss.item())\n",
    "\n",
    "gradients = []\n",
    "for param in target_model.parameters():\n",
    "    gradients.append(param.data.clone())\n",
    "# 清除参考模型的梯度\n",
    "optimizer_reference.zero_grad()\n",
    "\n",
    "# 清除目标模型的梯度\n",
    "optimizer_target.zero_grad()\n",
    "\n",
    "# 将参考模型的梯度应用到目标模型的参数上\n",
    "for i,param in enumerate(reference_model.parameters()):\n",
    "    param.data = gradients[i]\n",
    "\n",
    "for name, param in reference_model.named_parameters():\n",
    "        print(f\"Name: {name}, Shape: {param.data}\")\n",
    "for name, param in target_model.named_parameters():\n",
    "    print(f\"Name: {name}, Shape: {param.data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss1: 4.4983, Loss2: 4.4999, Total Loss: 8.9983\n",
      "Epoch [20/100], Loss1: 4.4982, Loss2: 4.4991, Total Loss: 8.9973\n",
      "Epoch [30/100], Loss1: 4.4966, Loss2: 4.4984, Total Loss: 8.9950\n",
      "Epoch [40/100], Loss1: 4.4953, Loss2: 4.4974, Total Loss: 8.9927\n",
      "Epoch [50/100], Loss1: 4.4951, Loss2: 4.4965, Total Loss: 8.9916\n",
      "Epoch [60/100], Loss1: 4.4923, Loss2: 4.4962, Total Loss: 8.9885\n",
      "Epoch [70/100], Loss1: 4.4907, Loss2: 4.4945, Total Loss: 8.9853\n",
      "Epoch [80/100], Loss1: 4.4858, Loss2: 4.4928, Total Loss: 8.9786\n",
      "Epoch [90/100], Loss1: 4.4813, Loss2: 4.4912, Total Loss: 8.9725\n",
      "Epoch [100/100], Loss1: 4.4670, Loss2: 4.4879, Total Loss: 8.9549\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 假设我们有两个相同的模型架构\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, input_size=91, hidden_size=10000, output_size=90+90):\n",
    "        super(MyModel, self).__init__()\n",
    "        # input_size = 91\n",
    "        # hidden_size = 10000\n",
    "        # output_size = 90*90\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)  # 输入层到隐藏层\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size)  # 批量归一化\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)  # 隐藏层到隐藏层\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_size)  # 批量归一化\n",
    "        self.fc3_1 = nn.Linear(hidden_size, output_size//2)  # 隐藏层到输出层,分类一\n",
    "        self.fc3_2 = nn.Linear(hidden_size, output_size//2)  # 隐藏层到输出层,分类二\n",
    "        self.dropout = nn.Dropout(p=0.5)  # 丢弃层\n",
    "\n",
    "    def forward(self, x):\n",
    "        #作用于(turn,state)\n",
    "        #x = torch.cat(torch.tensor(x[1]))\n",
    "        x = torch.relu(self.bn1(self.fc1(x)))  # 输入层\n",
    "        x = self.dropout(x)  # 丢弃层\n",
    "        x = torch.relu(self.bn2(self.fc2(x)))  # 隐藏层\n",
    "        y1 = F.softmax(self.fc3_1(x),dim=1)  # 输出层\n",
    "        y2 = F.softmax(self.fc3_2(x),dim=1)\n",
    "        return y1,y2\n",
    "\n",
    "# 实例化两个模型\n",
    "model = MyModel()\n",
    "\n",
    "# 创建优化器\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# 假设我们有一些输入数据\n",
    "inputs = torch.randn(100, 91)\n",
    "labels1 = torch.randint(90, (100,))\n",
    "labels2 = torch.randint(90, (100,))\n",
    "\n",
    "\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# 训练模型\n",
    "for epoch in range(100):\n",
    "    # 前向传播\n",
    "    outputs1, outputs2 = model(inputs)\n",
    "    \n",
    "    # 计算损失\n",
    "    loss1 = criterion(outputs1, labels1)\n",
    "    loss2 = criterion(outputs2, labels2)\n",
    "    total_loss = loss1 + loss2\n",
    "    \n",
    "    # 反向传播和优化\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/100], Loss1: {loss1.item():.4f}, Loss2: {loss2.item():.4f}, Total Loss: {total_loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tensor: tensor(3)\n",
      "Flattened vector: tensor([1, 2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 创建一个二维 Tensor\n",
    "tensor = torch.tensor(((1,2),(3,4)))\n",
    "print(\"Original tensor:\", torch.argmax(tensor))\n",
    "\n",
    "# 将 Tensor 展平成一维向量\n",
    "vector = tensor.view(-1)\n",
    "print(\"Flattened vector:\", vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated along dimension 0: tensor([1, 2, 3, 1])\n",
      "Concatenated along dimension 0 (for matrices): tensor([[1., 2.],\n",
      "        [3., 4.],\n",
      "        [5., 6.],\n",
      "        [7., 8.]])\n",
      "Concatenated along dimension 1 (for matrices): tensor([[1., 2., 5., 6.],\n",
      "        [3., 4., 7., 8.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 创建两个一维向量\n",
    "vec1 = torch.tensor([1, 2, 3])\n",
    "vec2 = torch.tensor([4, 5, 6])\n",
    "\n",
    "# 沿维度 0（默认为 0）拼接\n",
    "concat_vec = torch.cat((vec1, torch.tensor([1])))\n",
    "print(\"Concatenated along dimension 0:\", concat_vec)\n",
    "\n",
    "# 创建两个二维向量\n",
    "mat1 = torch.tensor([[1, 2], [3, 4]]).float()\n",
    "mat2 = torch.tensor([[5, 6], [7, 8]]).float()\n",
    "\n",
    "# 沿维度 0 拼接\n",
    "concat_mat_dim0 = torch.cat((mat1, mat2), dim=0)\n",
    "print(\"Concatenated along dimension 0 (for matrices):\", concat_mat_dim0)\n",
    "\n",
    "# 沿维度 1 拼接\n",
    "concat_mat_dim1 = torch.cat((mat1, mat2), dim=1)\n",
    "print(\"Concatenated along dimension 1 (for matrices):\", concat_mat_dim1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([1, 2, 3]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.0 (SDL 2.28.4, Python 3.11.9)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "times: 1 0 0\n",
      "times: 2 53 47\n",
      "times: 3 66 134\n",
      "times: 4 81 219\n",
      "times: 5 114 286\n",
      "times: 6 125 375\n",
      "times: 7 137 463\n",
      "times: 8 144 508\n",
      "times: 9 151 601\n",
      "times: 10 158 694\n",
      "times: 11 168 784\n",
      "times: 12 183 869\n",
      "times: 13 248 904\n",
      "times: 14 263 989\n",
      "times: 15 272 1080\n",
      "times: 16 280 1172\n",
      "times: 17 289 1263\n",
      "times: 18 335 1317\n",
      "times: 19 348 1404\n",
      "times: 20 373 1479\n",
      "times: 21 391 1561\n",
      "times: 22 400 1652\n",
      "times: 23 408 1744\n",
      "times: 24 413 1839\n",
      "times: 25 421 1931\n",
      "times: 26 426 2026\n",
      "times: 27 434 2118\n",
      "times: 28 436 2216\n",
      "times: 29 446 2306\n",
      "times: 30 459 2393\n",
      "times: 31 465 2487\n",
      "times: 32 477 2575\n",
      "times: 33 486 2666\n",
      "times: 34 498 2754\n",
      "times: 35 513 2839\n",
      "times: 36 524 2928\n",
      "times: 37 536 3016\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 28\u001b[0m\n\u001b[0;32m     26\u001b[0m trainer1 \u001b[38;5;241m=\u001b[39m ValueTrainGpu\u001b[38;5;241m.\u001b[39mTrainer(policy1, qnet1, qnetcopy1, robot1)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m):\n\u001b[1;32m---> 28\u001b[0m     \u001b[43mtrainer1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\workplace\\ML\\ValueTrainGpu.py:278\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, N, K, n_samples)\u001b[0m\n\u001b[0;32m    276\u001b[0m reward \u001b[38;5;241m=\u001b[39m sample[\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m    277\u001b[0m state1 \u001b[38;5;241m=\u001b[39m sample[\u001b[38;5;241m3\u001b[39m]\n\u001b[1;32m--> 278\u001b[0m y_q \u001b[38;5;241m=\u001b[39m reward \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq(state1, \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpi\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()) \u001b[38;5;66;03m#?\u001b[39;00m\n\u001b[0;32m    279\u001b[0m y_q\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer_qnet\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import ValueTrainGpu\n",
    "import Table\n",
    "table1 = Table.Table()\n",
    "table1.initial(1)\n",
    "# 初始化pygame\n",
    "# import pygame\n",
    "# pygame.init()\n",
    "\n",
    "# 设置窗口尺寸\n",
    "# screen_size = (800, 800)\n",
    "# screen = pygame.display.set_mode(screen_size)\n",
    "\n",
    "# # 设置窗口标题\n",
    "# pygame.display.set_caption('中国象棋')\n",
    "\n",
    "#table1.screenshow(screen)\n",
    "\n",
    "robot1 = ValueTrainGpu.Robot(table1)\n",
    "# samples = robot1.sampling(100)\n",
    "# robot1.sampleshow(samples,screen)\n",
    "#print(samples)\n",
    "\n",
    "policy1 = ValueTrainGpu.Policy()\n",
    "qnet1 = ValueTrainGpu.QNet()\n",
    "qnetcopy1 = ValueTrainGpu.QNet()\n",
    "trainer1 = ValueTrainGpu.Trainer(policy1, qnet1, qnetcopy1, robot1)\n",
    "for _ in range(100):\n",
    "    trainer1.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor(8, device='cuda:0'), tensor(1, device='cuda:0')),\n",
       " (tensor(2, device='cuda:0'), tensor(0, device='cuda:0')))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "tabletest = Table.Table()\n",
    "tabletest.initial(1)\n",
    "state = torch.cat((torch.tensor([tabletest.turn]).float(),torch.tensor(tabletest.table).view(-1).float())).to(\"cuda\")\n",
    "trainer1.pi(state)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
